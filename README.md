# 深層学習day2  
  
##  Section1:勾配消失問題  
  
要点  
・層が深くなると下層まで勾配が伝わらなくなるという問題がある。  
（例えば、sigmoid関数は、値が大きくなると勾配が小さくなってしまう。）  
・解決方法として、活性化関数にReLUを使う、重みの初期値の工夫、バッチ正規化がある。  
  
実装演習（2_2_2_vanishing_gradient_modified / 2_3_batch_normalization）  
・初期化を変える事で、精度が改善されるのを確認した。  
・batch normalizationの有無で、精度が変わるのを確認した。  
・pytorchのデフォルト実装にはHeの初期化が使われている模様。  
  
確認テスト  
・重みの初期値を0にすると、表現力が無くなる。  
・バッチ正規化を加える事で、収束が早くなり、精度も向上する。  
  
##  Section2:学習率最適化手法    
  
要点  
・勾配降下法において、パラメータが局所解に陥るのを防ぐ為にいくつかの手法が提案されている。  
・また、収束までのスピードも速くなるような工夫がある。  
・モメンタム、AdaGrad、RMSProp、Adamなどがある。
  
実装演習（2_4_optimizer.ipynb）  
・様々な最適化手法を実装。RMSProp、Adam以外はデフォルトでは学習が進まなかった。  
・学習率、活性化関数、重みの初期化方法、バッチ正規化を調整し、0.9546の精度を得た。  
  
確認テスト  
・モメンタム 前回の重みを考慮に入れる。  
・AdaGrad 学習率を時系列で減衰させる。単調減少。  
・RMSProp 学習率を減衰させる時に前回の値を使う。  
  
##  Section3:過学習  
  
要点  
・訓練データに最適化され、テストデータの精度があがらない状態＝過学習。  
・パラメータ数が多く、モデルの表現力が高すぎる場合に起きる。  
・またパラメータ数に比べて、データが少ない場合にも起きる。  
・回避法として、L1/L2正則化、ドロップアウト等があげられる。  
  
実装演習（2_5_overfiting.ipynb）  
・データを減らすことで過学習が起きる事を確認した。  
・正則化パラメータを調整し、過学習が軽減される事を確認した。  
・正則化が強すぎると学習しない事を確認した。  
  
確認テスト  
・L1正則化は変数が完全に0になるが、L2正則化は必ずしも0にならない。  
  
##  Section4:畳み込みニューラルネットワークの概念  
  
要点  
・全結合の代わりに畳み込み演算の層を使う=CNN。  
・パディング 画像の回りを一定の値で埋める。  
・ストライド フィルターの移動幅。  
・チャンネル 出力するフィルターの枚数。  
・間にプーリング層を挟む事がある。  
  
実装演習（2_6_simple_convolution_network.ipynb / 2_7_double_comvolution_network.ipynb）  
・CNNを実装し、MLPより高い精度を得た。  
・pooling層を減らす等、実験を行った。  
  
確認テスト  
・サイズ6×6の入力画像を、サイズ2×2のフィルタで  
畳み込んだ時の出力画像のサイズは６×６。  
  
##  Section5:最新のCNN  
  
要点  
・通常のCNN以外に、様々なタイプのCNNが開発されている。  
・AlexNetは、CNN+プーリング+全結合層。  
・過学習を防ぐ為に全結合層の出力にドロップアウトを使用。  
・最新のプーリングでは、Global Average Pooling等が用いられる。  
  
実装演習（2_8_deep_convolution_net.ipynb）  
・6層のCNNで98%の精度を得た。  
  
関連レポート
・現在は、EfficientNetが精度良く主流となっている。  
・自然言語処理の分野で活用されているTransformerが画像にも使われ始めている。  
  
